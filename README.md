# Language Model From Scratch

A project implementing language models from first principles, designed for learning and understanding the core concepts behind modern language models.
A non-official implement of CH1 assignment of Stanford CS336.
You can adjust params to train your own LLM from scratch.


## Overview

This repository contains code and documentation for building language models from scratch. The implementation focuses on clarity and educational value rather than performance optimization, making it ideal for those wanting to understand the inner workings of language models.

## Features

- Implementation of core language model architectures

## Installation

```bash
# Clone the repository
git clone https://github.com/DoubleRu1/language-model-from-scratch.git

# Navigate to the project directory
cd language-model-from-scratch

# Install dependencies using uv sync
uv sync
```

## Project Structure

```
language-model-from-scratch/
├── data/                  # Sample datasets(You can add your own data here) and vocab.
├── src/                   # Source code for the language model
│   ├── structure/         # Model architecture implementations
│   ├── tokenizer/         # Tokenization utilities
│   |
```


## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgments

- Papers and resources on transformer architecture and language modeling
- Open source implementations that inspired this educational project
